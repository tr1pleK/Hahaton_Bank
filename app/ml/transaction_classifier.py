"""–ú–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π"""
import pandas as pd
import numpy as np
import pickle
import os
from pathlib import Path
from typing import Optional, Tuple, Dict, Any
from datetime import datetime
import re

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from scipy.sparse import hstack

from app.models.category import TransactionCategory


# –ú–∞–ø–ø–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–∑ CSV –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å–∏—Å—Ç–µ–º—ã
CATEGORY_MAPPING = {
    'Rent': TransactionCategory.UTILITIES,
    'Misc': TransactionCategory.OTHER_EXPENSE,
    'Food': TransactionCategory.PRODUCTS,
    'Salary': TransactionCategory.SALARY,
    'Shopping': TransactionCategory.CLOTHING,
    'Transport': TransactionCategory.TRANSPORT,
}


class TransactionClassifier:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, model_path: Optional[str] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        
        Args:
            model_path: –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        self.model_path = model_path or self._get_default_model_path()
        self.model: Optional[Pipeline] = None
        self.vectorizer: Optional[TfidfVectorizer] = None
        self.scaler: Optional[StandardScaler] = None
        self.is_trained = False
        
        # –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å, –µ—Å–ª–∏ –æ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if os.path.exists(self.model_path):
            print(f"üîç –ù–∞–π–¥–µ–Ω–∞ –º–æ–¥–µ–ª—å –ø–æ –ø—É—Ç–∏: {self.model_path}")
            self.load_model()
            if self.is_trained:
                print(f"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")
            else:
                print(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–∞–π–¥–µ–Ω–∞, –Ω–æ –Ω–µ –ø–æ–º–µ—á–µ–Ω–∞ –∫–∞–∫ –æ–±—É—á–µ–Ω–Ω–∞—è")
        else:
            print(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –ø—É—Ç–∏: {self.model_path}")
            print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤—Å–µ–≥–¥–∞ 0.5)")
    
    @staticmethod
    def _get_default_model_path() -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –ø—É—Ç—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏"""
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å (—Ä–∞–±–æ—Ç–∞–µ—Ç –≤ Docker –∏ –Ω–∞ —Ö–æ—Å—Ç–µ)
        base_dir = Path(__file__).parent.parent.parent
        models_dir = base_dir / "ml_models"
        models_dir.mkdir(exist_ok=True)
        relative_path = str(models_dir / "transaction_classifier.pkl")
        
        # –ï—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ø–æ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–º—É –ø—É—Ç–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        if os.path.exists(relative_path):
            return relative_path
        
        # –î–ª—è Windows —Ö–æ—Å—Ç–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å
        if os.name == 'nt':  # Windows
            windows_path = r"C:\Users\–ï–≥–æ—Ä\IdeaProjects\Hahaton_Bank\backend\ml_models\transaction_classifier.pkl"
            if os.path.exists(windows_path):
                return windows_path
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å (–±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏)
        return relative_path
    
    def _preprocess_text(self, text: str) -> str:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è"""
        if pd.isna(text) or text is None:
            return ""
        
        text = str(text).lower()
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã –∏ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'[^a-z–∞-—è—ë0-9\s]', ' ', text)
        # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    def _extract_features(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –¥–∞–Ω–Ω—ã—Ö
        
        Returns:
            Tuple —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏ –º–µ—Ç–∫–∞–º–∏
        """
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è (RefNo)
        df['description_clean'] = df['RefNo'].apply(self._preprocess_text)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—É–º–º—ã —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ (Withdrawal –∏–ª–∏ Deposit)
        def get_amount(row):
            """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É–º–º—ã –∏–∑ —Å—Ç—Ä–æ–∫–∏"""
            try:
                withdrawal = row.get('Withdrawal', 0)
                deposit = row.get('Deposit', 0)
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —á–∏—Å–ª–æ, –µ—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞
                if pd.notna(withdrawal):
                    withdrawal = float(withdrawal) if withdrawal != '' else 0.0
                else:
                    withdrawal = 0.0
                    
                if pd.notna(deposit):
                    deposit = float(deposit) if deposit != '' else 0.0
                else:
                    deposit = 0.0
                
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—É–º–º—É (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ)
                if withdrawal > 0:
                    return withdrawal
                elif deposit > 0:
                    return deposit
                else:
                    return 0.0
            except (ValueError, TypeError):
                return 0.0
        
        df['amount'] = df.apply(get_amount, axis=1)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ (—Ä–∞—Å—Ö–æ–¥ –∏–ª–∏ –¥–æ—Ö–æ–¥)
        def get_is_expense(row):
            """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è —Ä–∞—Å—Ö–æ–¥–æ–º"""
            try:
                withdrawal = row.get('Withdrawal', 0)
                if pd.notna(withdrawal):
                    withdrawal = float(withdrawal) if withdrawal != '' else 0.0
                    return 1 if withdrawal > 0 else 0
                return 0
            except (ValueError, TypeError):
                return 0
        
        df['is_expense'] = df.apply(get_is_expense, axis=1)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –¥–∞—Ç—ã
        df['date'] = pd.to_datetime(df['Date'], errors='coerce', dayfirst=True, format='mixed')
        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ NaN –∑–Ω–∞—á–µ–Ω–∏–π –¥–∞—Ç—ã —Ç–µ–∫—É—â–µ–π –¥–∞—Ç–æ–π
        df['date'] = df['date'].fillna(pd.Timestamp.now())
        df['day_of_week'] = df['date'].dt.dayofweek
        df['day_of_month'] = df['date'].dt.day
        df['month'] = df['date'].dt.month
        
        # –ú–∞–ø–ø–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        df['category_mapped'] = df['Category'].map(CATEGORY_MAPPING)
        df = df.dropna(subset=['category_mapped'])
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å NaN –≤ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
        df = df.dropna(subset=['amount', 'day_of_week', 'day_of_month', 'month'])
        
        if len(df) == 0:
            raise ValueError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        
        # –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–∏
        X_text = df['description_clean'].values
        X_numeric = df[['amount', 'is_expense', 'day_of_week', 'day_of_month', 'month']].values
        y = df['category_mapped'].apply(lambda x: x.value).values
        
        return (X_text, X_numeric), y
    
    def _create_model(self) -> Pipeline:
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
        # TF-IDF –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        text_transformer = TfidfVectorizer(
            max_features=100,
            ngram_range=(1, 2),
            min_df=2,
            stop_words=None
        )
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        numeric_transformer = StandardScaler()
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä
        preprocessor = ColumnTransformer(
            transformers=[
                ('text', text_transformer, 0),
                ('numeric', numeric_transformer, 1)
            ],
            remainder='passthrough'
        )
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        classifier = RandomForestClassifier(
            n_estimators=100,
            max_depth=20,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω
        # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: ColumnTransformer —Ç—Ä–µ–±—É–µ—Ç –æ—Å–æ–±–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏ —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        # –ü–æ—ç—Ç–æ–º—É —Å–æ–∑–¥–∞–¥–∏–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
        return {
            'text_transformer': text_transformer,
            'numeric_transformer': numeric_transformer,
            'classifier': classifier
        }
    
    def train(self, csv_path: str, test_size: float = 0.2, force_retrain: bool = False) -> Dict[str, Any]:
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ CSV
        
        Args:
            csv_path: –ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏
            test_size: –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏
            force_retrain: –ï—Å–ª–∏ True, –ø–µ—Ä–µ–æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –¥–∞–∂–µ –µ—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è
        """
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
        if not force_retrain and os.path.exists(self.model_path):
            print(f"üì¶ –ú–æ–¥–µ–ª—å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ø–æ –ø—É—Ç–∏: {self.model_path}")
            print(f"   –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –º–æ–¥–µ–ª—å –≤–º–µ—Å—Ç–æ –æ–±—É—á–µ–Ω–∏—è –Ω–æ–≤–æ–π...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –º–æ–¥–µ–ª—å
            self.load_model()
            
            if self.is_trained and self.model is not None:
                print(f"‚úÖ –°—É—â–µ—Å—Ç–≤—É—é—â–∞—è –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                print(f"   –î–ª—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä force_retrain=True")
                
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
                return {
                    'message': '–ú–æ–¥–µ–ª—å —É–∂–µ –æ–±—É—á–µ–Ω–∞ –∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ —Ñ–∞–π–ª–∞',
                    'model_path': self.model_path,
                    'is_trained': True,
                    'loaded_from_file': True
                }
            else:
                print(f"‚ö†Ô∏è –§–∞–π–ª –º–æ–¥–µ–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–æ –º–æ–¥–µ–ª—å –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                print(f"   –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏...")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        df = pd.read_csv(csv_path)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫
        df = df.dropna(how='all')
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å –ø—É—Å—Ç—ã–º–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏ –∏–ª–∏ RefNo
        df = df.dropna(subset=['Category', 'RefNo'])
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        df = df[df['Category'].isin(CATEGORY_MAPPING.keys())]
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫, –≥–¥–µ –Ω–µ—Ç –Ω–∏ Withdrawal, –Ω–∏ Deposit
        df = df[
            (pd.notna(df['Withdrawal']) & (df['Withdrawal'] != 0)) | 
            (pd.notna(df['Deposit']) & (df['Deposit'] != 0))
        ]
        
        if len(df) == 0:
            raise ValueError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        (X_text, X_numeric), y = self._extract_features(df)
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
        X_text_train, X_text_test, X_numeric_train, X_numeric_test, y_train, y_test = train_test_split(
            X_text, X_numeric, y, test_size=test_size, random_state=42, stratify=y
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤
        self.vectorizer = TfidfVectorizer(
            max_features=100,
            ngram_range=(1, 2),
            min_df=2,
            stop_words=None
        )
        X_text_train_tfidf = self.vectorizer.fit_transform(X_text_train)
        X_text_test_tfidf = self.vectorizer.transform(X_text_test)
        
        self.scaler = StandardScaler()
        X_numeric_train_scaled = self.scaler.fit_transform(X_numeric_train)
        X_numeric_test_scaled = self.scaler.transform(X_numeric_test)
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        X_train = hstack([X_text_train_tfidf, X_numeric_train_scaled])
        X_test = hstack([X_text_test_tfidf, X_numeric_test_scaled])
        
        # –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        self.model = RandomForestClassifier(
            n_estimators=100,
            max_depth=20,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        self.model.fit(X_train, y_train)
        
        # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
        y_pred = self.model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        report = classification_report(y_test, y_pred, output_dict=True)
        
        self.is_trained = True
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        self.save_model()
        
        return {
            'accuracy': accuracy,
            'classification_report': report,
            'train_samples': len(y_train),
            'test_samples': len(y_test)
        }
    
    def predict(self, description: str, amount: float, is_expense: bool = True, 
                date: Optional[datetime] = None) -> TransactionCategory:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
        
        Args:
            description: –û–ø–∏—Å–∞–Ω–∏–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ (RefNo –∏–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–µ)
            amount: –°—É–º–º–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
            is_expense: –Ø–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è —Ä–∞—Å—Ö–æ–¥–æ–º
            date: –î–∞—Ç–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è
        """
        if not self.is_trained or self.model is None:
            # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é —ç–≤—Ä–∏—Å—Ç–∏–∫—É
            return self._fallback_classify(description, amount, is_expense)
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        text_clean = self._preprocess_text(description)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –¥–∞—Ç—ã
        if date is None:
            date = datetime.now()
        
        day_of_week = date.weekday()
        day_of_month = date.day
        month = date.month
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
        text_tfidf = self.vectorizer.transform([text_clean])
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        numeric_features = np.array([[amount, 1 if is_expense else 0, day_of_week, day_of_month, month]])
        numeric_scaled = self.scaler.transform(numeric_features)
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        X = hstack([text_tfidf, numeric_scaled])
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        prediction = self.model.predict(X)[0]
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ –≤ TransactionCategory
        for cat in TransactionCategory:
            if cat.value == prediction:
                return cat
        
        return TransactionCategory.OTHER_EXPENSE
    
    def predict_with_probability(
        self, 
        description: str, 
        amount: float, 
        is_expense: bool = True, 
        date: Optional[datetime] = None
    ) -> Tuple[TransactionCategory, float]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é
        
        Args:
            description: –û–ø–∏—Å–∞–Ω–∏–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ (RefNo –∏–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–µ)
            amount: –°—É–º–º–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
            is_expense: –Ø–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è —Ä–∞—Å—Ö–æ–¥–æ–º
            date: –î–∞—Ç–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            Tuple —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π –∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é
        """
        if not self.is_trained or self.model is None:
            # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é —ç–≤—Ä–∏—Å—Ç–∏–∫—É
            category = self._fallback_classify(description, amount, is_expense)
            return category, 0.5  # –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è fallback
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        text_clean = self._preprocess_text(description)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –¥–∞—Ç—ã
        if date is None:
            date = datetime.now()
        
        day_of_week = date.weekday()
        day_of_month = date.day
        month = date.month
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
        text_tfidf = self.vectorizer.transform([text_clean])
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        numeric_features = np.array([[amount, 1 if is_expense else 0, day_of_week, day_of_month, month]])
        numeric_scaled = self.scaler.transform(numeric_features)
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        X = hstack([text_tfidf, numeric_scaled])
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏
        probabilities = self.model.predict_proba(X)
        prediction = self.model.predict(X)[0]
        
        # probabilities –º–æ–∂–µ—Ç –±—ã—Ç—å 2D –º–∞—Å—Å–∏–≤–æ–º (n_samples, n_classes)
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É (–ø–µ—Ä–≤—ã–π –æ–±—Ä–∞–∑–µ—Ü)
        if probabilities.ndim > 1:
            prob_array = probabilities[0]
        else:
            prob_array = probabilities
        
        # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ –≤ model.classes_
        # model.classes_ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ –∫–ª–∞—Å—Å—ã –≤ —Ç–æ–º –∂–µ –ø–æ—Ä—è–¥–∫–µ, —á—Ç–æ –∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        prediction_idx = None
        if hasattr(self.model, 'classes_'):
            try:
                prediction_idx = np.where(self.model.classes_ == prediction)[0]
                if len(prediction_idx) > 0:
                    prediction_idx = prediction_idx[0]
                else:
                    prediction_idx = None
            except Exception:
                prediction_idx = None
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
        if prediction_idx is not None and prediction_idx < len(prob_array):
            max_probability = float(prob_array[prediction_idx])
        else:
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∏–Ω–¥–µ–∫—Å, –±–µ—Ä–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
            max_probability = float(np.max(prob_array))
        
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0, 1]
        # predict_proba –≤—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, –Ω–æ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
        max_probability = max(0.0, min(1.0, max_probability))
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ –≤ TransactionCategory
        for cat in TransactionCategory:
            if cat.value == prediction:
                return cat, max_probability
        
        return TransactionCategory.OTHER_EXPENSE, max_probability
    
    def predict_from_dataframe(self, df: pd.DataFrame) -> Tuple[TransactionCategory, float]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ DataFrame (–∫–∞–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)
        
        Args:
            df: DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ Date, RefNo, Withdrawal, Deposit, Balance
            
        Returns:
            Tuple —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π –∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é
        """
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–æ–¥–µ–ª–∏
        if not self.is_trained:
            print(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞ (is_trained=False), –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback")
        if self.model is None:
            print(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (model=None), –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback")
        if self.vectorizer is None:
            print(f"‚ö†Ô∏è Vectorizer –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback")
        if self.scaler is None:
            print(f"‚ö†Ô∏è Scaler –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback")
        
        if not self.is_trained or self.model is None or self.vectorizer is None or self.scaler is None:
            # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é —ç–≤—Ä–∏—Å—Ç–∏–∫—É
            if len(df) == 0:
                return TransactionCategory.OTHER_EXPENSE, 0.5
            
            row = df.iloc[0]
            description = str(row.get('RefNo', ''))
            amount = float(row.get('Withdrawal', 0) or row.get('Deposit', 0) or 0)
            is_expense = float(row.get('Withdrawal', 0) or 0) > 0
            category = self._fallback_classify(description, amount, is_expense)
            print(f"üîÑ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ fallback –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: {category.value} (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: 0.5)")
            return category, 0.5
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—É –∂–µ –ª–æ–≥–∏–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —á—Ç–æ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è (RefNo)
        df['description_clean'] = df['RefNo'].apply(self._preprocess_text)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—É–º–º—ã —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ (Withdrawal –∏–ª–∏ Deposit)
        def get_amount(row):
            """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É–º–º—ã –∏–∑ —Å—Ç—Ä–æ–∫–∏"""
            try:
                withdrawal = row.get('Withdrawal', 0)
                deposit = row.get('Deposit', 0)
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —á–∏—Å–ª–æ, –µ—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞
                if pd.notna(withdrawal):
                    withdrawal = float(withdrawal) if withdrawal != '' else 0.0
                else:
                    withdrawal = 0.0
                    
                if pd.notna(deposit):
                    deposit = float(deposit) if deposit != '' else 0.0
                else:
                    deposit = 0.0
                
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—É–º–º—É (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ)
                if withdrawal > 0:
                    return withdrawal
                elif deposit > 0:
                    return deposit
                else:
                    return 0.0
            except (ValueError, TypeError):
                return 0.0
        
        df['amount'] = df.apply(get_amount, axis=1)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ (—Ä–∞—Å—Ö–æ–¥ –∏–ª–∏ –¥–æ—Ö–æ–¥)
        def get_is_expense(row):
            """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è —Ä–∞—Å—Ö–æ–¥–æ–º"""
            try:
                withdrawal = row.get('Withdrawal', 0)
                if pd.notna(withdrawal):
                    withdrawal = float(withdrawal) if withdrawal != '' else 0.0
                    return 1 if withdrawal > 0 else 0
                return 0
            except (ValueError, TypeError):
                return 0
        
        df['is_expense'] = df.apply(get_is_expense, axis=1)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –¥–∞—Ç—ã
        df['date'] = pd.to_datetime(df['Date'], errors='coerce', dayfirst=True, format='mixed')
        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ NaN –∑–Ω–∞—á–µ–Ω–∏–π –¥–∞—Ç—ã —Ç–µ–∫—É—â–µ–π –¥–∞—Ç–æ–π
        df['date'] = df['date'].fillna(pd.Timestamp.now())
        df['day_of_week'] = df['date'].dt.dayofweek
        df['day_of_month'] = df['date'].dt.day
        df['month'] = df['date'].dt.month
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å NaN –≤ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
        df = df.dropna(subset=['amount', 'day_of_week', 'day_of_month', 'month'])
        
        if len(df) == 0:
            return TransactionCategory.OTHER_EXPENSE, 0.5
        
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        row = df.iloc[0]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø vectorizer
        from sklearn.preprocessing import LabelEncoder
        from sklearn.feature_extraction.text import TfidfVectorizer
        
        text_clean = row['description_clean']
        refno_original = str(row.get('RefNo', ''))
        
        # –ï—Å–ª–∏ vectorizer —ç—Ç–æ LabelEncoder, –∑–Ω–∞—á–∏—Ç –º–æ–¥–µ–ª—å –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ –ø–æ-–¥—Ä—É–≥–æ–º—É
        # LabelEncoder –∫–æ–¥–∏—Ä—É–µ—Ç RefNo –∫–∞–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
        if isinstance(self.vectorizer, LabelEncoder):
            try:
                # –ü—Ä–æ–±—É–µ–º –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å RefNo —á–µ—Ä–µ–∑ LabelEncoder
                # –ï—Å–ª–∏ RefNo –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (0 –∏–ª–∏ -1)
                if hasattr(self.vectorizer, 'classes_'):
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ RefNo –≤ –æ–±—É—á–µ–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–∞—Ö
                    if refno_original in self.vectorizer.classes_:
                        refno_encoded = self.vectorizer.transform([refno_original])[0]
                    else:
                        # –ï—Å–ª–∏ RefNo –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ + 1 –∏–ª–∏ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                        # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º -1 –∫–∞–∫ –º–∞—Ä–∫–µ—Ä –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
                        # –ú–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –±—ã–ª–∞ –±—ã—Ç—å –æ–±—É—á–µ–Ω–∞ —Å —É—á–µ—Ç–æ–º —Ç–∞–∫–∏—Ö —Å–ª—É—á–∞–µ–≤
                        if len(self.vectorizer.classes_) > 0:
                            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ + 1
                            # –∏–ª–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å -1 –∫–∞–∫ –º–∞—Ä–∫–µ—Ä –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–≥–æ
                            max_encoded = len(self.vectorizer.classes_) - 1
                            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–µ–º–Ω–æ–≥–æ –±–æ–ª—å—à–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ
                            refno_encoded = max_encoded + 1
                        else:
                            refno_encoded = 0
                else:
                    # –ï—Å–ª–∏ –Ω–µ—Ç classes_, –ø—Ä–æ–±—É–µ–º transform –Ω–∞–ø—Ä—è–º—É—é
                    try:
                        refno_encoded = self.vectorizer.transform([refno_original])[0]
                    except ValueError:
                        # –ï—Å–ª–∏ transform –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º 0
                        refno_encoded = 0
                
                # LabelEncoder –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–¥–Ω–æ —á–∏—Å–ª–æ, –Ω—É–∂–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ –º–∞—Å—Å–∏–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                # –í –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–æ–≥–æ, –∫–∞–∫ –º–æ–¥–µ–ª—å –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞, —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–æ—Å—Ç–æ —á–∏—Å–ª–æ
                # –∏–ª–∏ –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å one-hot encoding
                # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∫–∞–∫ –ø—Ä–∏–∑–Ω–∞–∫
                refno_feature = np.array([[float(refno_encoded)]])
                
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏ RefNo —á–µ—Ä–µ–∑ LabelEncoder: {e}")
                print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è")
                description = str(row.get('RefNo', ''))
                amount = float(row.get('amount', 0))
                is_expense = bool(row.get('is_expense', 0))
                category = self._fallback_classify(description, amount, is_expense)
                return category, 0.5
        
        # –ï—Å–ª–∏ vectorizer —ç—Ç–æ TfidfVectorizer, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥
        elif isinstance(self.vectorizer, TfidfVectorizer):
            try:
                text_tfidf = self.vectorizer.transform([text_clean])
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
                description = str(row.get('RefNo', ''))
                amount = float(row.get('amount', 0))
                is_expense = bool(row.get('is_expense', 0))
                category = self._fallback_classify(description, amount, is_expense)
                return category, 0.5
        else:
            # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø vectorizer
            print(f"‚ö†Ô∏è –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ç–∏–ø vectorizer: {type(self.vectorizer)}")
            print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è")
            description = str(row.get('RefNo', ''))
            amount = float(row.get('amount', 0))
            is_expense = bool(row.get('is_expense', 0))
            category = self._fallback_classify(description, amount, is_expense)
            return category, 0.5
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        numeric_features = np.array([[ 
            row['amount'],
            row['is_expense'],
            row['day_of_week'],
            row['day_of_month'],
            row['month']
        ]])
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø scaler
        from sklearn.preprocessing import StandardScaler
        if isinstance(self.scaler, StandardScaler):
            try:
                numeric_scaled = self.scaler.transform(numeric_features)
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
                description = str(row.get('RefNo', ''))
                amount = float(row.get('amount', 0))
                is_expense = bool(row.get('is_expense', 0))
                category = self._fallback_classify(description, amount, is_expense)
                return category, 0.5
        elif isinstance(self.scaler, list):
            # –ï—Å–ª–∏ scaler —ç—Ç–æ list, –≤–æ–∑–º–æ–∂–Ω–æ —ç—Ç–æ —Å–ø–∏—Å–æ–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –±–µ–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∏–ª–∏ –ø—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–æ—Å—Ç—É—é –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é
            print(f"   Scaler –∏–º–µ–µ—Ç —Ç–∏–ø list, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –±–µ–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è")
            numeric_scaled = numeric_features
        else:
            # –ï—Å–ª–∏ scaler —ç—Ç–æ –Ω–µ StandardScaler, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –±–µ–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
            print(f"   Scaler –∏–º–µ–µ—Ç –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ç–∏–ø: {type(self.scaler)}, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –±–µ–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è")
            numeric_scaled = numeric_features
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ vectorizer
        if isinstance(self.vectorizer, LabelEncoder):
            # –î–ª—è LabelEncoder –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π RefNo —Å —á–∏—Å–ª–æ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            # refno_feature —É–∂–µ —è–≤–ª—è–µ—Ç—Å—è –º–∞—Å—Å–∏–≤–æ–º —Ñ–æ—Ä–º—ã (1, 1)
            # numeric_scaled –∏–º–µ–µ—Ç —Ñ–æ—Ä–º—É (1, 5)
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏—Ö –ø–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª–∏
            X = np.hstack([refno_feature, numeric_scaled])
        elif isinstance(self.vectorizer, TfidfVectorizer):
            # –î–ª—è TfidfVectorizer –æ–±—ä–µ–¥–∏–Ω—è–µ–º TF-IDF –≤–µ–∫—Ç–æ—Ä—ã —Å —á–∏—Å–ª–æ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            X = hstack([text_tfidf, numeric_scaled])
        else:
            # –ï—Å–ª–∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            print(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø vectorizer, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏")
            X = numeric_scaled
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏
        try:
            probabilities = self.model.predict_proba(X)
            prediction = self.model.predict(X)[0]
            
            # probabilities –º–æ–∂–µ—Ç –±—ã—Ç—å 2D –º–∞—Å—Å–∏–≤–æ–º (n_samples, n_classes)
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É (–ø–µ—Ä–≤—ã–π –æ–±—Ä–∞–∑–µ—Ü)
            if probabilities.ndim > 1:
                prob_array = probabilities[0]
            else:
                prob_array = probabilities
            
            # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ –≤ model.classes_
            # model.classes_ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ –∫–ª–∞—Å—Å—ã –≤ —Ç–æ–º –∂–µ –ø–æ—Ä—è–¥–∫–µ, —á—Ç–æ –∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            prediction_idx = None
            if hasattr(self.model, 'classes_'):
                try:
                    prediction_idx = np.where(self.model.classes_ == prediction)[0]
                    if len(prediction_idx) > 0:
                        prediction_idx = prediction_idx[0]
                    else:
                        prediction_idx = None
                except Exception as e:
                    print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –∏–Ω–¥–µ–∫—Å–∞ –∫–ª–∞—Å—Å–∞: {e}")
                    prediction_idx = None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
            if prediction_idx is not None and prediction_idx < len(prob_array):
                max_probability = float(prob_array[prediction_idx])
            else:
                # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∏–Ω–¥–µ–∫—Å, –±–µ—Ä–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
                max_probability = float(np.max(prob_array))
            
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0, 1]
            # predict_proba –≤—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, –Ω–æ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
            max_probability = max(0.0, min(1.0, max_probability))
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ (–º–æ–∂–Ω–æ —É–±—Ä–∞—Ç—å –ø–æ—Å–ª–µ –ø—Ä–æ–≤–µ—Ä–∫–∏)
            if hasattr(self.model, 'classes_'):
                print(f"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {prediction}, –∏–Ω–¥–µ–∫—Å: {prediction_idx}, –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {max_probability:.4f}")
                print(f"   –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–ª–∞—Å—Å—ã: {self.model.classes_}")
                print(f"   –í—Å–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: {prob_array}")
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ –≤ TransactionCategory
            for cat in TransactionCategory:
                if cat.value == prediction:
                    return cat, max_probability
            
            return TransactionCategory.OTHER_EXPENSE, max_probability
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏: {e}")
            description = str(row.get('RefNo', ''))
            amount = float(row.get('amount', 0))
            is_expense = bool(row.get('is_expense', 0))
            category = self._fallback_classify(description, amount, is_expense)
            return category, 0.5
    
    def _fallback_classify(self, description: str, amount: float, is_expense: bool) -> TransactionCategory:
        """–ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞"""
        desc_lower = (description or "").lower()
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        if any(word in desc_lower for word in ["food", "–ø—Ä–æ–¥—É–∫—Ç—ã", "–º–∞–≥–∞–∑–∏–Ω", "–µ–¥–∞", "grocery"]):
            return TransactionCategory.PRODUCTS
        if any(word in desc_lower for word in ["transport", "—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç", "–º–µ—Ç—Ä–æ", "—Ç–∞–∫—Å–∏", "uber"]):
            return TransactionCategory.TRANSPORT
        if any(word in desc_lower for word in ["rent", "–∞—Ä–µ–Ω–¥–∞", "–∫–æ–º–º—É–Ω–∞–ª—å–Ω—ã–µ"]):
            return TransactionCategory.UTILITIES
        if any(word in desc_lower for word in ["salary", "–∑–∞—Ä–ø–ª–∞—Ç–∞", "payroll"]):
            return TransactionCategory.SALARY
        if any(word in desc_lower for word in ["shopping", "–ø–æ–∫—É–ø–∫–∏", "–º–∞–≥–∞–∑–∏–Ω"]):
            return TransactionCategory.CLOTHING
        
        # –ë–æ–ª—å—à–∏–µ —Å—É–º–º—ã - –≤–æ–∑–º–æ–∂–Ω–æ –¥–æ—Ö–æ–¥
        if not is_expense and amount > 10000:
            return TransactionCategory.SALARY
        
        return TransactionCategory.OTHER_EXPENSE
    
    def save_model(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ —Ñ–∞–π–ª"""
        if self.model is None:
            return
        
        model_data = {
            'model': self.model,
            'vectorizer': self.vectorizer,
            'scaler': self.scaler,
            'is_trained': self.is_trained,
            'trained_at': datetime.now().isoformat()
        }
        
        with open(self.model_path, 'wb') as f:
            pickle.dump(model_data, f)
    
    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
        if not os.path.exists(self.model_path):
            print(f"‚ùå –§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {self.model_path}")
            return
        
        try:
            print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑: {self.model_path}")
            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –≤–µ—Ä—Å–∏–π
            import warnings
            with warnings.catch_warnings():
                warnings.filterwarnings("ignore", category=UserWarning)
                with open(self.model_path, 'rb') as f:
                    model_data = pickle.load(f)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö (–º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ª–æ–≤–∞—Ä—å –∏–ª–∏ –∫–æ—Ä—Ç–µ–∂)
            print(f"   –¢–∏–ø –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {type(model_data)}")
            
            if isinstance(model_data, dict):
                # –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç - —Å–ª–æ–≤–∞—Ä—å
                print(f"   –§–æ—Ä–º–∞—Ç: —Å–ª–æ–≤–∞—Ä—å (dict)")
                self.model = model_data.get('model')
                self.vectorizer = model_data.get('vectorizer')
                self.scaler = model_data.get('scaler')
                self.is_trained = model_data.get('is_trained', False)
            elif isinstance(model_data, tuple):
                # –°—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç - –∫–æ—Ä—Ç–µ–∂
                print(f"   –§–æ—Ä–º–∞—Ç: –∫–æ—Ä—Ç–µ–∂ (tuple), –¥–ª–∏–Ω–∞: {len(model_data)}")
                print(f"   –¢–∏–ø—ã —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {[type(x).__name__ for x in model_data]}")
                
                # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Ñ–æ—Ä–º–∞—Ç–∞ –∫–æ—Ä—Ç–µ–∂–∞
                if len(model_data) >= 3:
                    # –í–∞—Ä–∏–∞–Ω—Ç 1: (model, vectorizer, scaler)
                    self.model = model_data[0]
                    self.vectorizer = model_data[1]
                    self.scaler = model_data[2]
                    self.is_trained = True
                    print(f"   –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω —Ñ–æ—Ä–º–∞—Ç: (model, vectorizer, scaler)")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø—ã –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
                    from sklearn.preprocessing import LabelEncoder
                    from sklearn.feature_extraction.text import TfidfVectorizer
                    from sklearn.preprocessing import StandardScaler
                    
                    if isinstance(self.vectorizer, LabelEncoder):
                        print(f"   ‚ÑπÔ∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è LabelEncoder –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è RefNo (–∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è)")
                        print(f"   –ú–æ–¥–µ–ª—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É –Ω–æ–≤—ã—Ö RefNo —á–µ—Ä–µ–∑ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
                    
                    if not isinstance(self.scaler, StandardScaler):
                        print(f"   ‚ÑπÔ∏è Scaler –∏–º–µ–µ—Ç —Ç–∏–ø: {type(self.scaler)}, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –±–µ–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è")
                elif len(model_data) == 2:
                    # –í–∞—Ä–∏–∞–Ω—Ç 2: –≤–æ–∑–º–æ–∂–Ω–æ (model, vectorizer) –∏–ª–∏ —á—Ç–æ-—Ç–æ –¥—Ä—É–≥–æ–µ
                    print(f"   ‚ö†Ô∏è –ö–æ—Ä—Ç–µ–∂ –∏–∑ 2 —ç–ª–µ–º–µ–Ω—Ç–æ–≤, –ø—Ä–æ–±—É–µ–º –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å...")
                    # –ü—Ä–æ–±—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–æ —Ç–∏–ø–∞–º
                    for i, item in enumerate(model_data):
                        item_type = type(item).__name__
                        if 'RandomForest' in item_type or 'Classifier' in item_type:
                            self.model = item
                            print(f"   –ù–∞–π–¥–µ–Ω model –≤ –ø–æ–∑–∏—Ü–∏–∏ {i}")
                        elif 'Tfidf' in item_type or 'Vectorizer' in item_type:
                            self.vectorizer = item
                            print(f"   –ù–∞–π–¥–µ–Ω vectorizer –≤ –ø–æ–∑–∏—Ü–∏–∏ {i}")
                        elif 'Scaler' in item_type:
                            self.scaler = item
                            print(f"   –ù–∞–π–¥–µ–Ω scaler –≤ –ø–æ–∑–∏—Ü–∏–∏ {i}")
                    self.is_trained = True
                else:
                    raise ValueError(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –∫–æ—Ä—Ç–µ–∂–∞: –æ–∂–∏–¥–∞–ª–æ—Å—å 2-3 —ç–ª–µ–º–µ–Ω—Ç–∞, –ø–æ–ª—É—á–µ–Ω–æ {len(model_data)}")
            else:
                print(f"   ‚ö†Ô∏è –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç: {type(model_data)}")
                # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∞—Ç—Ä–∏–±—É—Ç—ã –Ω–∞–ø—Ä—è–º—É—é, –µ—Å–ª–∏ —ç—Ç–æ –æ–±—ä–µ–∫—Ç
                if hasattr(model_data, 'model'):
                    self.model = model_data.model
                if hasattr(model_data, 'vectorizer'):
                    self.vectorizer = model_data.vectorizer
                if hasattr(model_data, 'scaler'):
                    self.scaler = model_data.scaler
                if hasattr(model_data, 'is_trained'):
                    self.is_trained = model_data.is_trained
                else:
                    self.is_trained = self.model is not None and self.vectorizer is not None and self.scaler is not None
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
            if self.model is None:
                print(f"‚ùå –ú–æ–¥–µ–ª—å (classifier) –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ñ–∞–π–ª–µ")
                self.is_trained = False
            elif self.vectorizer is None:
                print(f"‚ùå Vectorizer –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Ñ–∞–π–ª–µ")
                self.is_trained = False
            elif self.scaler is None:
                print(f"‚ùå Scaler –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Ñ–∞–π–ª–µ")
                self.is_trained = False
            else:
                print(f"‚úÖ –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")
                print(f"   is_trained: {self.is_trained}")
                print(f"   model type: {type(self.model)}")
                print(f"   vectorizer type: {type(self.vectorizer)}")
                print(f"   scaler type: {type(self.scaler)}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            import traceback
            traceback.print_exc()
            self.model = None
            self.vectorizer = None
            self.scaler = None
            self.is_trained = False

